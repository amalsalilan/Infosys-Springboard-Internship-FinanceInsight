{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNSyw3aBgIbY",
        "outputId": "f531247a-9c9d-4bd6-85c2-3c6ffef004db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langextract\n",
            "  Downloading langextract-1.0.9-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.4.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (3.12.15)\n",
            "Collecting async_timeout>=4.0.0 (from langextract)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting exceptiongroup>=1.1.0 (from langextract)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: google-genai>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.38.0)\n",
            "Collecting ml-collections>=0.1.0 (from langextract)\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: more-itertools>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (10.8.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.2.2)\n",
            "Requirement already satisfied: pydantic>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.11.9)\n",
            "Requirement already satisfied: python-dotenv>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.1.1)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (4.15.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (11.3.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.20.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (4.10.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (2025.8.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai>=0.1.0->langextract) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai>=0.1.0->langextract) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai>=0.1.0->langextract) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->langextract) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (0.6.1)\n",
            "Downloading langextract-1.0.9-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-docx, pypdf, ml-collections, exceptiongroup, async_timeout, python-pptx, langextract\n",
            "Successfully installed XlsxWriter-3.2.9 async_timeout-5.0.1 exceptiongroup-1.3.0 langextract-1.0.9 ml-collections-1.1.0 pypdf-6.1.1 python-docx-1.2.0 python-pptx-1.0.2\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install langextract pypdf python-docx python-pptx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import json\n",
        "import textwrap\n",
        "from typing import Dict, List, Optional, Union, Tuple\n",
        "from google.colab import files\n",
        "from pypdf import PdfReader\n",
        "import docx\n",
        "from pptx import Presentation\n",
        "import langextract as lx\n",
        "from langextract import data as lx_data"
      ],
      "metadata": {
        "id": "bULFv42UgM-d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set API key\n",
        "os.environ[\"LANGEXTRACT_API_KEY\"] = \"AIzaSyCTRrMUxRJRot5tX-eCSjbo433M2dayPjs\""
      ],
      "metadata": {
        "id": "w2fSb0nIg40w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LangExtractFinancialEntityExtractor:\n",
        "    def __init__(self):\n",
        "        # Initialize LangExtract\n",
        "        self.prompt = textwrap.dedent(\"\"\"\\\n",
        "            Extract the following entities in order of appearance:\n",
        "            - ORGANIZATION (company, bank, firm, institution)\n",
        "            - PERSON (individuals named in the document)\n",
        "            - DATE (contract dates, deadlines, durations)\n",
        "            - MONEY (loans, amounts, salaries, payments)\n",
        "            - PERCENT (interest rates, growth rates, percentages)\n",
        "            - ACCOUNT_NUMBER (bank accounts, policy numbers, transaction IDs)\n",
        "            - ROLE (positions like Director, Manager, Borrower, Lender)\n",
        "            - CONTRACT_REFERENCE (Agreement, Clause, Section references)\n",
        "            - LOCATION (cities, offices, addresses)\n",
        "        \"\"\")\n",
        "\n",
        "        # Define example document for LangExtract\n",
        "        self.example_document = lx_data.Document(\n",
        "            text=\"\"\"\n",
        "            This Strategic Partnership Agreement is executed on March 1, 2023, between TechNova Solutions Pvt. Ltd.,\n",
        "            a software development company based in Hyderabad, India, and GlobalEdge Analytics Inc., a U.S.-based data analytics firm\n",
        "            with offices in New York City, USA.\n",
        "\n",
        "            Under this agreement, GlobalEdge will invest USD 2,000,000 in TechNova for the development of a joint AI research lab.\n",
        "            The funds will be transferred to Account Number 5544332211 maintained with Axis Bank, Banjara Hills Branch, Hyderabad.\n",
        "\n",
        "            The agreement will be valid for a term of 3 years starting from April 1, 2023, and is subject to renewal upon mutual consent.\n",
        "            TechNova's CEO, Mr. Ramesh Varma, and GlobalEdge's Director of Partnerships, Ms. Linda Zhao, have signed the agreement\n",
        "            on behalf of their respective companies.\n",
        "\n",
        "            The profit-sharing ratio has been agreed at 60% (TechNova) and 40% (GlobalEdge).\n",
        "            Agreement Reference Code: PARTNER-GE-TN-2023-0301.\n",
        "            Any disputes will be resolved under the jurisdiction of the Telangana High Court.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Define extractions separately\n",
        "        self.example_extractions = [\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"DATE\",\n",
        "                extraction_text=\"March 1, 2023\",\n",
        "                attributes={\"type\": \"signing_date\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"ORG\",\n",
        "                extraction_text=\"TechNova Solutions Pvt. Ltd.\",\n",
        "                attributes={\"industry\": \"software\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"ORG\",\n",
        "                extraction_text=\"GlobalEdge Analytics Inc.\",\n",
        "                attributes={\"industry\": \"data analytics\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"LOCATION\",\n",
        "                extraction_text=\"Hyderabad, India\",\n",
        "                attributes={\"type\": \"company_headquarters\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"LOCATION\",\n",
        "                extraction_text=\"New York City, USA\",\n",
        "                attributes={\"type\": \"global_office\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"MONEY\",\n",
        "                extraction_text=\"USD 2,000,000\",\n",
        "                attributes={\"purpose\": \"AI research investment\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"ACCOUNT_NUMBER\",\n",
        "                extraction_text=\"5544332211\",\n",
        "                attributes={\"bank\": \"Axis Bank\", \"branch\": \"Banjara Hills\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"DATE\",\n",
        "                extraction_text=\"April 1, 2023\",\n",
        "                attributes={\"type\": \"agreement_start_date\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"PERSON\",\n",
        "                extraction_text=\"Mr. Ramesh Varma\",\n",
        "                attributes={\"role\": \"CEO\", \"organization\": \"TechNova\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"PERSON\",\n",
        "                extraction_text=\"Ms. Linda Zhao\",\n",
        "                attributes={\"role\": \"Director of Partnerships\", \"organization\": \"GlobalEdge\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"PERCENT\",\n",
        "                extraction_text=\"60%\",\n",
        "                attributes={\"entity\": \"TechNova\", \"type\": \"profit_share\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"PERCENT\",\n",
        "                extraction_text=\"40%\",\n",
        "                attributes={\"entity\": \"GlobalEdge\", \"type\": \"profit_share\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"CONTRACT_REFERENCE\",\n",
        "                extraction_text=\"PARTNER-GE-TN-2023-0301\",\n",
        "                attributes={\"type\": \"partnership_agreement_id\"}\n",
        "            ),\n",
        "            lx_data.Extraction(\n",
        "                extraction_class=\"LOCATION\",\n",
        "                extraction_text=\"Telangana High Court\",\n",
        "                attributes={\"type\": \"jurisdiction\"}\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        self.adani_companies = [\n",
        "            \"Adani Enterprises Limited\", \"Adani Ports and Special Economic Zone Limited\",\n",
        "            \"Adani Power Limited\", \"Adani Transmission Limited\", \"Adani Gas Limited\",\n",
        "            \"Adani Green Energy Limited\", \"Adani Total Gas Limited\", \"Adani Energy Solutions Limited\",\n",
        "            \"Adani Airport Holdings Limited\", \"Adani Roads Transport Limited\", \"Adani New Industries Limited\",\n",
        "            \"Adani Wilmar Limited\", \"Ambuja Cements Limited\", \"ACC Limited\"\n",
        "        ]\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF file.\"\"\"\n",
        "        reader = PdfReader(pdf_path)\n",
        "        return \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
        "\n",
        "    def extract_text_from_docx(self, docx_path: str) -> str:\n",
        "        \"\"\"Extract text from DOCX file.\"\"\"\n",
        "        doc = docx.Document(docx_path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
        "\n",
        "    def extract_text_from_ppt(self, ppt_path: str) -> str:\n",
        "        \"\"\"Extract text from PPT/PPTX file.\"\"\"\n",
        "        prs = Presentation(ppt_path)\n",
        "        text = \"\"\n",
        "        for slide in prs.slides:\n",
        "            for shape in slide.shapes:\n",
        "                if hasattr(shape, \"text\"):\n",
        "                    text += shape.text + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Preprocess the extracted text.\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.replace(\"₹\", \"INR \").replace(\"$\", \"USD \")\n",
        "        text = ''.join(c for c in text if c in string.printable)\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_with_langextract(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using LangExtract API.\"\"\"\n",
        "        try:\n",
        "            # Create a document with extractions\n",
        "            example_doc_with_extractions = lx_data.Document(\n",
        "                text=self.example_document.text,\n",
        "                extractions=self.example_extractions\n",
        "            )\n",
        "\n",
        "            # Use LangExtract to extract entities\n",
        "            result = lx.extract(\n",
        "                text_or_documents=text,\n",
        "                prompt_description=self.prompt,\n",
        "                examples=[example_doc_with_extractions],\n",
        "                model_id=\"gemini-2.5-flash\",\n",
        "                extraction_passes=3,\n",
        "                max_workers=20,\n",
        "                max_char_buffer=1000\n",
        "            )\n",
        "\n",
        "            # Initialize entity categories\n",
        "            entities = {\n",
        "                \"company_names\": [],\n",
        "                \"financial_events\": [],\n",
        "                \"stock_prices\": [],\n",
        "                \"revenue\": [],\n",
        "                \"market_cap\": [],\n",
        "                \"earnings\": [],\n",
        "                \"financial_ratios\": [],\n",
        "                \"financial_dates\": [],\n",
        "                \"phone_numbers\": []\n",
        "            }\n",
        "\n",
        "            # Process extractions\n",
        "            for extraction in result.extractions:\n",
        "                entity_class = extraction.extraction_class\n",
        "                entity_text = extraction.extraction_text\n",
        "\n",
        "                # Map LangExtract classes to our categories\n",
        "                if entity_class == \"ORG\":\n",
        "                    entities[\"company_names\"].append(entity_text)\n",
        "                elif entity_class == \"MONEY\":\n",
        "                    # Categorize monetary values\n",
        "                    if any(keyword in entity_text.lower() for keyword in [\"revenue\", \"income\", \"sales\"]):\n",
        "                        entities[\"revenue\"].append(entity_text)\n",
        "                    elif any(keyword in entity_text.lower() for keyword in [\"profit\", \"loss\", \"earnings\", \"pat\"]):\n",
        "                        entities[\"earnings\"].append(entity_text)\n",
        "                    elif any(keyword in entity_text.lower() for keyword in [\"market cap\", \"capitalization\"]):\n",
        "                        entities[\"market_cap\"].append(entity_text)\n",
        "                    elif any(keyword in entity_text.lower() for keyword in [\"price\", \"share\"]):\n",
        "                        entities[\"stock_prices\"].append(entity_text)\n",
        "                    else:\n",
        "                        # Default to revenue if no specific category matches\n",
        "                        entities[\"revenue\"].append(entity_text)\n",
        "                elif entity_class == \"PERCENT\":\n",
        "                    entities[\"financial_ratios\"].append(entity_text)\n",
        "                elif entity_class == \"DATE\":\n",
        "                    entities[\"financial_dates\"].append(entity_text)\n",
        "                elif entity_class == \"ACCOUNT_NUMBER\":\n",
        "                    # Check if it's a phone number\n",
        "                    if re.match(r\"\\+?\\d{1,3}[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\", entity_text):\n",
        "                        entities[\"phone_numbers\"].append(entity_text)\n",
        "                elif entity_class == \"CONTRACT_REFERENCE\":\n",
        "                    # Treat contract references as financial events\n",
        "                    entities[\"financial_events\"].append(f\"Reference: {entity_text}\")\n",
        "\n",
        "            return entities\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting with LangExtract: {e}\")\n",
        "            # Return empty entities if extraction fails\n",
        "            return {\n",
        "                \"company_names\": [],\n",
        "                \"financial_events\": [],\n",
        "                \"stock_prices\": [],\n",
        "                \"revenue\": [],\n",
        "                \"market_cap\": [],\n",
        "                \"earnings\": [],\n",
        "                \"financial_ratios\": [],\n",
        "                \"financial_dates\": [],\n",
        "                \"phone_numbers\": []\n",
        "            }\n",
        "\n",
        "    def extract_company_names(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract company names from the text using LangExtract.\"\"\"\n",
        "        # First, add known Adani companies if found in text\n",
        "        companies = []\n",
        "        for company in self.adani_companies:\n",
        "            if company.lower() in text.lower():\n",
        "                companies.append(company)\n",
        "\n",
        "        # Use LangExtract to find more companies\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        for company in langextract_entities[\"company_names\"]:\n",
        "            if company not in companies:\n",
        "                companies.append(company)\n",
        "\n",
        "        return list(set(companies))\n",
        "\n",
        "    def extract_financial_events(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract financial events from the text using LangExtract.\"\"\"\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        return langextract_entities[\"financial_events\"]\n",
        "\n",
        "    def extract_stock_prices(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract stock prices from the text using LangExtract.\"\"\"\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        return langextract_entities[\"stock_prices\"]\n",
        "\n",
        "    def extract_revenue(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract revenue information from the text using LangExtract.\"\"\"\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        return langextract_entities[\"revenue\"]\n",
        "\n",
        "    def extract_market_cap(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract market capitalization from the text using LangExtract.\"\"\"\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        return langextract_entities[\"market_cap\"]\n",
        "\n",
        "    def extract_earnings(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract earnings information from the text using LangExtract.\"\"\"\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        return langextract_entities[\"earnings\"]\n",
        "\n",
        "    def extract_financial_ratios(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract financial ratios from the text using LangExtract.\"\"\"\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        return langextract_entities[\"financial_ratios\"]\n",
        "\n",
        "    def extract_financial_dates(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract financial dates from the text using LangExtract.\"\"\"\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        return langextract_entities[\"financial_dates\"]\n",
        "\n",
        "    def extract_phone_numbers(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract phone numbers from the text using LangExtract.\"\"\"\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "        return langextract_entities[\"phone_numbers\"]\n",
        "\n",
        "    def process_document(self, file_path: str) -> Dict[str, Union[str, List[str]]]:\n",
        "        \"\"\"Process a document and extract financial entities using LangExtract.\"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            return {\"error\": f\"File does not exist: {file_path}\"}\n",
        "\n",
        "        if file_path.lower().endswith(\".pdf\"):\n",
        "            raw_text = self.extract_text_from_pdf(file_path)\n",
        "        elif file_path.lower().endswith(\".docx\"):\n",
        "            raw_text = self.extract_text_from_docx(file_path)\n",
        "        elif file_path.lower().endswith((\".ppt\", \".pptx\")):\n",
        "            raw_text = self.extract_text_from_ppt(file_path)\n",
        "        else:\n",
        "            return {\"error\": \"Unsupported file format. Please use PDF, DOCX, or PPT/PPTX.\"}\n",
        "\n",
        "        if not raw_text.strip():\n",
        "            return {\"error\": \"No text could be extracted from the document.\"}\n",
        "\n",
        "        text = self.preprocess_text(raw_text)\n",
        "\n",
        "        # Extract all entities using LangExtract\n",
        "        langextract_entities = self.extract_with_langextract(text)\n",
        "\n",
        "        # Extract all entities\n",
        "        company_names = self.extract_company_names(text)\n",
        "        financial_events = langextract_entities[\"financial_events\"]\n",
        "        stock_prices = langextract_entities[\"stock_prices\"]\n",
        "        revenue = langextract_entities[\"revenue\"]\n",
        "        market_cap = langextract_entities[\"market_cap\"]\n",
        "        earnings = langextract_entities[\"earnings\"]\n",
        "        financial_ratios = langextract_entities[\"financial_ratios\"]\n",
        "        financial_dates = langextract_entities[\"financial_dates\"]\n",
        "        phone_numbers = langextract_entities[\"phone_numbers\"]\n",
        "\n",
        "        result = {\n",
        "            \"file_path\": file_path,\n",
        "            \"text_length\": len(text),\n",
        "            \"entities\": {\n",
        "                \"company_names\": company_names,\n",
        "                \"financial_events\": financial_events,\n",
        "                \"stock_prices\": stock_prices,\n",
        "                \"revenue\": revenue,\n",
        "                \"market_cap\": market_cap,\n",
        "                \"earnings\": earnings,\n",
        "                \"financial_ratios\": financial_ratios,\n",
        "                \"financial_dates\": financial_dates,\n",
        "                \"phone_numbers\": phone_numbers\n",
        "            }\n",
        "        }\n",
        "        return result"
      ],
      "metadata": {
        "id": "aoDSzn7pg-EJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize extractor\n",
        "extractor = LangExtractFinancialEntityExtractor()"
      ],
      "metadata": {
        "id": "Q_ibMoe_hVg-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload and process document\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    results = extractor.process_document(filename)\n",
        "\n",
        "    if \"error\" in results:\n",
        "        print(f\"Error: {results['error']}\")\n",
        "    else:\n",
        "        print(f\"\\nDocument Analysis Results\")\n",
        "        print(f\"Text Length: {results['text_length']} characters\")\n",
        "\n",
        "        for entity_type, entities in results[\"entities\"].items():\n",
        "            print(f\"\\n{entity_type.replace('_', ' ').title()} ({len(entities)} found)\")\n",
        "            if entities:\n",
        "                for entity in entities[:10]:\n",
        "                    print(f\"- {entity}\")\n",
        "            else:\n",
        "                print(\"No entities found.\")"
      ],
      "metadata": {
        "id": "AEUXAGzuqLld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "63b14217-2cdf-4ac6-8288-5a40204cbe6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-04ab0b8f-e296-4206-aa2d-67779a8bf493\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-04ab0b8f-e296-4206-aa2d-67779a8bf493\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Q1 FY26.pdf to Q1 FY26.pdf\n",
            "Error extracting with LangExtract: Document.__init__() got an unexpected keyword argument 'extractions'\n",
            "Error extracting with LangExtract: Document.__init__() got an unexpected keyword argument 'extractions'\n",
            "\n",
            "Document Analysis Results\n",
            "Text Length: 46472 characters\n",
            "\n",
            "Company Names (10 found)\n",
            "- Adani Total Gas Limited\n",
            "- Adani Energy Solutions Limited\n",
            "- Adani New Industries Limited\n",
            "- Adani Enterprises Limited\n",
            "- Adani Ports and Special Economic Zone Limited\n",
            "- Adani Airport Holdings Limited\n",
            "- Adani Roads Transport Limited\n",
            "- Adani Green Energy Limited\n",
            "- ACC Limited\n",
            "- Adani Power Limited\n",
            "\n",
            "Financial Events (0 found)\n",
            "No entities found.\n",
            "\n",
            "Stock Prices (0 found)\n",
            "No entities found.\n",
            "\n",
            "Revenue (0 found)\n",
            "No entities found.\n",
            "\n",
            "Market Cap (0 found)\n",
            "No entities found.\n",
            "\n",
            "Earnings (0 found)\n",
            "No entities found.\n",
            "\n",
            "Financial Ratios (0 found)\n",
            "No entities found.\n",
            "\n",
            "Financial Dates (0 found)\n",
            "No entities found.\n",
            "\n",
            "Phone Numbers (0 found)\n",
            "No entities found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "if 'results' in locals() and \"error\" not in results:\n",
        "    output_filename = \"langextract_financial_entities.json\"\n",
        "    with open(output_filename, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    files.download(output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ey4St8pPipPX",
        "outputId": "b1ca11fc-81f6-42ba-e558-1148f0d90d2a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c5ac3059-0cc2-4d71-901c-6cf1f43e1ab7\", \"langextract_financial_entities.json\", 664)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}