{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja-8bBlfw0fq"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported\")\n",
        "\n",
        "# %%\n",
        "model_name = 'bert-base-uncased'\n",
        "num_labels = 3\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "num_epochs = 3\n",
        "learning_rate = 2e-5\n",
        "seed = 42\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# %%\n",
        "texts = [\n",
        "    \"Company reports record quarterly profits exceeding expectations\",\n",
        "    \"Stock market reaches all-time high with strong gains\",\n",
        "    \"Federal Reserve announces stimulus boosting investor confidence\",\n",
        "    \"Tech company shares surge 8% after innovation announcement\",\n",
        "    \"Strong job growth indicates economic recovery\",\n",
        "    \"Merger announcement sends stocks soaring\",\n",
        "    \"Bank adoption of cryptocurrency signals mainstream acceptance\",\n",
        "    \"GDP growth exceeds forecasts showing economic expansion\",\n",
        "    \"Company files for bankruptcy amid mounting debts\",\n",
        "    \"Market crashes as recession fears grip investors\",\n",
        "    \"Unemployment rate rises sharply concerning economists\",\n",
        "    \"Major bank reports significant losses from bad loans\",\n",
        "    \"Trade war escalates causing market volatility\",\n",
        "    \"Company shares plummet 20% after product recall\",\n",
        "    \"Economic indicators point to potential recession\",\n",
        "    \"Corporate earnings disappoint leading to selloff\",\n",
        "    \"Federal Reserve maintains interest rates unchanged\",\n",
        "    \"Company releases earnings in line with expectations\",\n",
        "    \"Trading volume remains steady as investors wait\",\n",
        "    \"Board announces regular quarterly dividend\",\n",
        "    \"Market closes mixed with minor changes\",\n",
        "    \"Company announces CEO transition\",\n",
        "    \"Annual shareholder meeting scheduled\",\n",
        "    \"Regulatory filing submitted for compliance\",\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    0, 0, 0, 0, 0, 0, 0, 0,\n",
        "    1, 1, 1, 1, 1, 1, 1, 1,\n",
        "    2, 2, 2, 2, 2, 2, 2, 2\n",
        "]\n",
        "\n",
        "texts = texts * 20\n",
        "labels = labels * 20\n",
        "\n",
        "df = pd.DataFrame({'text': texts, 'label': labels})\n",
        "print(f\"Dataset size: {len(df)}\")\n",
        "\n",
        "# %%\n",
        "label_counts = df['label'].value_counts().sort_index()\n",
        "print(\"Label distribution:\")\n",
        "print(f\"Positive (0): {label_counts[0]}\")\n",
        "print(f\"Negative (1): {label_counts[1]}\")\n",
        "print(f\"Neutral (2): {label_counts[2]}\")\n",
        "\n",
        "# %%\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['text'].values,\n",
        "    df['label'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=seed,\n",
        "    stratify=df['label'].values\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "\n",
        "# %%\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "print(f\"Tokenizer loaded\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# %%\n",
        "class FinancialDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"Dataset class defined\")\n",
        "\n",
        "# %%\n",
        "train_dataset = FinancialDataset(X_train, y_train, tokenizer, max_length)\n",
        "val_dataset = FinancialDataset(X_val, y_val, tokenizer, max_length)\n",
        "\n",
        "print(f\"Train dataset created: {len(train_dataset)} samples\")\n",
        "print(f\"Validation dataset created: {len(val_dataset)} samples\")\n",
        "\n",
        "# %%\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "print(\"BERT model loaded\")\n",
        "print(f\"Model type: {model_name}\")\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "# %%\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=2,\n",
        "    seed=seed,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "print(\"Training arguments set\")\n",
        "\n",
        "# %%\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"Trainer created\")\n",
        "\n",
        "# %%\n",
        "print(\"Starting training...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# %%\n",
        "train_loss = train_result.metrics['train_loss']\n",
        "print(f\"Final training loss: {train_loss:.4f}\")\n",
        "\n",
        "# %%\n",
        "print(\"Evaluating on validation set...\")\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"Evaluation results:\")\n",
        "print(f\"Validation loss: {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "# %%\n",
        "print(\"Getting predictions...\")\n",
        "\n",
        "predictions = trainer.predict(val_dataset)\n",
        "\n",
        "predicted_probs = predictions.predictions\n",
        "predicted_classes = np.argmax(predicted_probs, axis=1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "print(f\"Predictions shape: {predicted_classes.shape}\")\n",
        "print(f\"True labels shape: {true_labels.shape}\")\n",
        "\n",
        "# %%\n",
        "correct_predictions = 0\n",
        "total_predictions = len(predicted_classes)\n",
        "\n",
        "for i in range(len(predicted_classes)):\n",
        "    if predicted_classes[i] == true_labels[i]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# %%\n",
        "confusion_matrix_values = np.zeros((3, 3), dtype=int)\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "    true_label = true_labels[i]\n",
        "    predicted_label = predicted_classes[i]\n",
        "    confusion_matrix_values[true_label][predicted_label] += 1\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix_values)\n",
        "\n",
        "# %%\n",
        "label_names = ['Positive', 'Negative', 'Neutral']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix_values, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "print(\"\\nPer-class metrics:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for class_idx in range(3):\n",
        "    tp = confusion_matrix_values[class_idx][class_idx]\n",
        "\n",
        "    fp = 0\n",
        "    for i in range(3):\n",
        "        if i != class_idx:\n",
        "            fp += confusion_matrix_values[i][class_idx]\n",
        "\n",
        "    fn = 0\n",
        "    for j in range(3):\n",
        "        if j != class_idx:\n",
        "            fn += confusion_matrix_values[class_idx][j]\n",
        "\n",
        "    if (tp + fp) > 0:\n",
        "        precision = tp / (tp + fp)\n",
        "    else:\n",
        "        precision = 0\n",
        "\n",
        "    if (tp + fn) > 0:\n",
        "        recall = tp / (tp + fn)\n",
        "    else:\n",
        "        recall = 0\n",
        "\n",
        "    if (precision + recall) > 0:\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0\n",
        "\n",
        "    print(f\"\\n{label_names[class_idx]}:\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "# %%\n",
        "test_sentences = [\n",
        "    \"Tech company announces breakthrough, stock jumps 12%\",\n",
        "    \"Global recession fears cause market crash\",\n",
        "    \"Company maintains steady growth as expected\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting on new examples:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    inputs = tokenizer(\n",
        "        sentence,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        predicted_class = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = torch.max(probs).item()\n",
        "\n",
        "    print(f\"\\nText: {sentence}\")\n",
        "    print(f\"Predicted class: {label_names[predicted_class]}\")\n",
        "    print(f\"Confidence: {confidence:.4f}\")\n",
        "\n",
        "    print(\"All probabilities:\")\n",
        "    print(f\"  Positive: {probs[0][0].item():.4f}\")\n",
        "    print(f\"  Negative: {probs[0][1].item():.4f}\")\n",
        "    print(f\"  Neutral: {probs[0][2].item():.4f}\")\n",
        "\n",
        "# %%\n",
        "training_loss_history = []\n",
        "validation_loss_history = []\n",
        "\n",
        "for log in trainer.state.log_history:\n",
        "    if 'loss' in log:\n",
        "        training_loss_history.append(log['loss'])\n",
        "    if 'eval_loss' in log:\n",
        "        validation_loss_history.append(log['eval_loss'])\n",
        "\n",
        "if len(validation_loss_history) > 0:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    epochs = range(1, len(validation_loss_history) + 1)\n",
        "    plt.plot(epochs, validation_loss_history, 'o-', label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss per Epoch')\n",
        "    plt.grid(True)\n",
        "\n",
        "    if len(training_loss_history) > 0:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        steps = range(1, len(training_loss_history) + 1)\n",
        "        plt.plot(steps, training_loss_history, label='Training Loss')\n",
        "        plt.xlabel('Training Steps')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss Progress')\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# %%\n",
        "model_save_path = './financial_bert_model'\n",
        "\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "# %%\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(model_save_path)\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained(model_save_path)\n",
        "\n",
        "loaded_model = loaded_model.to(device)\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "test_text = \"Stock market shows positive momentum\"\n",
        "\n",
        "test_inputs = loaded_tokenizer(\n",
        "    test_text,\n",
        "    return_tensors='pt',\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "test_inputs = {k: v.to(device) for k, v in test_inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_outputs = loaded_model(**test_inputs)\n",
        "    test_logits = test_outputs.logits\n",
        "    test_probs = torch.softmax(test_logits, dim=-1)\n",
        "    test_prediction = torch.argmax(test_probs, dim=-1).item()\n",
        "\n",
        "print(f\"\\nTest with loaded model:\")\n",
        "print(f\"Text: {test_text}\")\n",
        "print(f\"Prediction: {label_names[test_prediction]}\")\n",
        "\n",
        "# %%\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of epochs: {num_epochs}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Final validation accuracy: {accuracy:.2%}\")\n",
        "print(f\"Final training loss: {train_loss:.4f}\")\n",
        "print(f\"Final validation loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# %%\n",
        "print(\"Notebook completed successfully!\")"
      ]
    }
  ]
}